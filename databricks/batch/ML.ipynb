{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e0f8e6-d555-4f97-aa32-8c8052fb0089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_gold = 'abfss://datalake@dls0tfm.dfs.core.windows.net/gold/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6a32ae-7381-458f-a2c5-d8c909357f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, LinearRegressionModel, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07a90f1-3d74-45a2-afb0-42489619cdd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ganancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8297d5e3-e566-4ee5-8fda-c8a9192db683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dias = spark.read.format('delta').load(f'{path_gold}dias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2832bfef-271b-4667-ae64-1a89743b60aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[fecha: date, dia_semana: int, mes: int, ano: int, tipo: string, ganancia: double, tavg: double, tmin: double, tmax: double, prcp: double, snow: int, wspd: double, wpgt: double, pres: double, tsun: double, processing_date: date]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dias.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ffb8ec-d420-4903-9eb5-99e866028880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols = [field for (field, dataType) in df_dias.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37c355-7991-4855-bf5b-79e666d5e891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['dia_semana',\n",
       " 'mes',\n",
       " 'ano',\n",
       " 'tavg',\n",
       " 'tmin',\n",
       " 'tmax',\n",
       " 'prcp',\n",
       " 'snow',\n",
       " 'wspd',\n",
       " 'wpgt',\n",
       " 'pres',\n",
       " 'tsun']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols = [field for (field, dataType) in df_dias.dtypes if dataType in ('int','double') and field not in ['ganancia']]\n",
    "numerical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3c5817-814a-4b35-8680-bbcd44c1e4eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df_dias.randomSplit([.8, .2], seed=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1083a4-debf-478d-9bb6-635a3dc10ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler_inputs = index_output_cols + numerical_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bbf350-25c3-47b3-9b78-84e3e0df2f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Regresión multilineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b88367d-c9de-41a6-aadb-915e1110b2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='ganancia')\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a61a25ce-bebb-4bf6-bcba-1557ab98cbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nepsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: ganancia)\nloss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "#print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ea0b66-9b52-41e8-bdba-addad649ce8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parámetros\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.maxIter, [50, 75, 100]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ganancia', predictionCol='prediction')\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid, numFolds=3, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d0f68e-f54e-451d-b1f4-34221b53d7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 375.7152300967988\nR2: 0.5181991650598177\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:11:45 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ce475edd34475c93a8f4d38d7f7466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:12:18 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/68fa43c66a9247d7a9b2cd5201d0ec0b/artifacts/full_pipeline/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3e7c1697c9486ea3d7c0cae58c7625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:12:33 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e9f3ae82b147cc8f0ce58cd2a7a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:13:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/68fa43c66a9247d7a9b2cd5201d0ec0b/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a633881c3c84cec94c980e2bb254f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" # la segunda parte es el formato en que queremos el datetime\n",
    "run_name = f\"ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    pipeline_model = cv.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = pipeline_model.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    avg_metrics = pipeline_model.avgMetrics\n",
    "    param_maps = pipeline_model.getEstimatorParamMaps()\n",
    "    for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "        mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    mlflow.spark.log_model(pipeline_model, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = pipeline_model.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    lr_model = best_model.stages[-1]\n",
    "    assert isinstance(lr_model, LinearRegressionModel)\n",
    "    coef_list = lr_model.coefficients.toArray().tolist()\n",
    "    intercept = lr_model.intercept\n",
    "    for i, coef in enumerate(coef_list):\n",
    "        mlflow.log_param(f\"best_coef_{i}\", coef)\n",
    "    mlflow.log_param(\"best_intercept\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528d00d4-7cf1-4ebe-9358-e6c624e5b8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repetimos el experimento sin considerar la variable tsun, debido a su coeficiente cercano a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d4085d-72a7-4dcb-ad83-b9709210aa90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler_inputs2 = assembler_inputs[:-1]\n",
    "vec_assembler2 = VectorAssembler(inputCols=assembler_inputs2, outputCol=\"features\")\n",
    "lr2 = LinearRegression(featuresCol='features', labelCol='ganancia')\n",
    "pipeline2 = Pipeline(stages=[string_indexer, vec_assembler2, lr2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1875a896-dc6b-461f-ab4b-fba902b95a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv2 = CrossValidator(estimator=pipeline2, evaluator=evaluator, estimatorParamMaps=param_grid, numFolds=3, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7382fa62-f47a-4028-9665-1d2dca116dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 375.915680229751\nR2: 0.5176849308931606\nTipo: <class 'pyspark.ml.tuning.CrossValidatorModel'>\nTiene bestModel: True\nTipo de bestModel: <class 'pyspark.ml.pipeline.PipelineModel'>\nEtapas del bestModel: [StringIndexerModel: uid=StringIndexer_15355b634383, handleInvalid=skip, numInputCols=1, numOutputCols=1, VectorAssembler_76eab07e8882, LinearRegressionModel: uid=LinearRegression_99a76127d68e, numFeatures=12]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:42:16 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ea6ce50fbe49b1a607c700f98c667e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:42:50 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/96bcb5e6b11946ae93a5ce107f17e34f/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae65a2cb84b4b769cc01ade9bafe853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "run_name = f\"ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    pipeline_model2 = cv2.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = pipeline_model2.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    #avg_metrics = pipeline_model.avgMetrics\n",
    "    #param_maps = pipeline_model.getEstimatorParamMaps()\n",
    "    #for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "    #    mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "    print(\"Tipo:\", type(pipeline_model2))\n",
    "    print(\"Tiene bestModel:\", hasattr(pipeline_model2, \"bestModel\"))\n",
    "\n",
    "    if hasattr(pipeline_model2, \"bestModel\"):\n",
    "        print(\"Tipo de bestModel:\", type(pipeline_model2.bestModel))\n",
    "    \n",
    "    # ¿Tiene etapas?\n",
    "    if hasattr(pipeline_model2.bestModel, \"stages\"):\n",
    "        print(\"Etapas del bestModel:\", pipeline_model2.bestModel.stages)\n",
    "    else:\n",
    "        print(\" El bestModel no tiene etapas (no es un PipelineModel)\")\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    #mlflow.spark.log_model(pipeline_model2, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = pipeline_model2.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    lr_model = best_model.stages[-1]\n",
    "    assert isinstance(lr_model, LinearRegressionModel)\n",
    "    coef_list = lr_model.coefficients.toArray().tolist()\n",
    "    intercept = lr_model.intercept\n",
    "    for i, coef in enumerate(coef_list):\n",
    "        mlflow.log_param(f\"best_coef_{i}\", coef)\n",
    "    mlflow.log_param(\"best_intercept\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef32fd52-299f-4501-a311-93d3a7b79ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5861dda8-6552-4f4f-b83b-15c3cf8e5c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='ganancia')\n",
    "pipeline_rf = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10, 15]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .build()\\\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ganancia', predictionCol='prediction')\n",
    "\n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator, estimatorParamMaps=param_grid_rf, numFolds=3, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6133bc7-9d3e-4684-a201-e7a818da7c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 344.9016365030823\nR2: 0.5939865149863153\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:56:31 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69ad8c1769249dcb62a4d97758c9055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:57:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/7d592424886e47d9a5cd94aa96d704e8/artifacts/full_pipeline/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aabc12f6c0048b9918733f1e839e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:57:23 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a313a4190e194ab5bde374606ce262bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 13:57:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/7d592424886e47d9a5cd94aa96d704e8/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30339624e43405d94b7527920cc2961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"rf_ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    cv_model = cv_rf.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = cv_model.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    avg_metrics = cv_model.avgMetrics\n",
    "    param_maps = cv_model.getEstimatorParamMaps()\n",
    "    for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "        mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    mlflow.spark.log_model(cv_model, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = cv_model.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794bac80-a024-426a-8d7e-fd7c5d16638c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='ganancia')\n",
    "pipeline_rf = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10, 15]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .build()\\\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ganancia', predictionCol='prediction')\n",
    "\n",
    "cv_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator, estimatorParamMaps=param_grid_rf, numFolds=3, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3a2bfc-2498-4757-b09c-687f0d4272bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 344.9016365030823\nR2: 0.5939865149863153\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 13:06:55 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254276052b4a493fb2035ec8719b84f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 13:07:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/b568173a55fe4859a06758fd82d02227/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac309e1c6f046c29ec06a6e405bda2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"rf_ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    cv_model = cv_rf.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = cv_model.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    avg_metrics = cv_model.avgMetrics\n",
    "    param_maps = cv_model.getEstimatorParamMaps()\n",
    "    for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "        mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    #mlflow.spark.log_model(cv_model, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = cv_model.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "501fb6ef-4782-483a-ae7e-d711b00d78e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9091e8b1-fc3c-4c10-909b-3b9c655b3b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gb = GBTRegressor(featuresCol='features', labelCol='ganancia')\n",
    "pipeline_gb = Pipeline(stages=[string_indexer, vec_assembler, gb])\n",
    "param_grid_gb = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10, 15]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .build()\\\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ganancia', predictionCol='prediction')\n",
    "\n",
    "cv_gb = CrossValidator(estimator=pipeline_gb, evaluator=evaluator, estimatorParamMaps=param_grid_gb, numFolds=3, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5418f1-b5f3-4f5f-bec5-39b87a47e4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 342.3719605771597\nR2: 0.5999204733647583\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 13:58:37 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb30f39bab34e0f98efaea081b713f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 13:59:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/b66caf8248ba432aaa9accaa6df392fe/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de19c5e2218f4e91b87d2f5d4c77962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02989911 0.13811919 0.11238584 0.24369005 0.06169668 0.07094247\n 0.04540735 0.03349771 0.00786691 0.0545483  0.05197947 0.09447522\n 0.05549171]\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"gb_ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    cv_model = cv_gb.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = cv_model.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    avg_metrics = cv_model.avgMetrics\n",
    "    param_maps = cv_model.getEstimatorParamMaps()\n",
    "    for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "        mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    #mlflow.spark.log_model(cv_model, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = cv_model.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")\n",
    "\n",
    "    gbt = cv_model.bestModel.stages[-1]\n",
    "    importances = gbt.featureImportances.toArray()\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "877f573c-c4f7-4ed5-974b-4cb2ea2211cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lo que más usa, año, día de la semana y mes. Luego, presión, temperatura máxima y media. Vamos a probar sólo con esas.\n",
    "assembler_inputs_new = ['tavg', 'tmin', 'tmax', 'pres', 'dia_semana', 'mes', 'ano']\n",
    "vec_assembler_new = VectorAssembler(inputCols=assembler_inputs_new, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3a67ff-70e7-4467-9f6e-a9329fe88864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gb = GBTRegressor(featuresCol='features', labelCol='ganancia')\n",
    "pipeline_gb_new = Pipeline(stages=[vec_assembler_new, gb])\n",
    "param_grid_gb = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10, 15]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .build()\\\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ganancia', predictionCol='prediction')\n",
    "\n",
    "cv_gb_new = CrossValidator(estimator=pipeline_gb_new, evaluator=evaluator, estimatorParamMaps=param_grid_gb, numFolds=3, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd82d27c-514b-4079-a768-247ebe3b4f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 347.4313109343063\nR2: 0.5880088774280954\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 14:32:22 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f31b4f783a47c4a34dfc54d2a5f744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 14:32:59 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/3554617355181843/fd1f4cb6e662400a9a447aa601b07253/artifacts/best_model/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f351588c446b42ef94ec10aea9bc505b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07821231 0.10213065 0.08651189 0.15977164 0.17734735 0.13019292\n 0.26583325]\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"gb_ganancia_{datetime.now().strftime('%Y%m%d_%H%M%S')}\" \n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Entrenamiento\n",
    "    cv_model = cv_gb_new.fit(train_df)\n",
    "\n",
    "    # Predicciones\n",
    "    pred_df = cv_model.transform(test_df) # transform es equivalente al predict de scikit-learn\n",
    "\n",
    "    # Métricas\n",
    "    rmse = evaluator.evaluate(pred_df)\n",
    "    r2_evaluator = evaluator.copy({}).setMetricName(\"r2\")\n",
    "    r2 = r2_evaluator.evaluate(pred_df)\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R2: {r2}\")\n",
    "\n",
    "    # Logging a MLflow\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Logging hiperparámetros (opcional)\n",
    "    avg_metrics = cv_model.avgMetrics\n",
    "    param_maps = cv_model.getEstimatorParamMaps()\n",
    "    for i, (params, metric) in enumerate(zip(param_maps, avg_metrics)):\n",
    "        mlflow.log_metric(f\"fold_{i}_metric\", metric)\n",
    "\n",
    "   # Guardamos el pipeline completo, que incluye al mejor modelo y las transformaciones del pipeline, necesarias para la predicción\n",
    "    #mlflow.spark.log_model(cv_model, \"full_pipeline\")\n",
    "\n",
    "    # Opcional: Logging del mejor modelo (última etapa del pipeline contiene el CV), vamos a necesitar el pipeline para predecir en cualquier caso\n",
    "    best_model = cv_model.bestModel\n",
    "    mlflow.spark.log_model(best_model, \"best_model\")\n",
    "\n",
    "    gbt = cv_model.bestModel.stages[-1]\n",
    "    importances = gbt.featureImportances.toArray()\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb89a9c-7c55-4b01-8dde-6c45012f3de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Predicción con gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2020c89-d249-425d-bdda-9a55e00453ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5830067c0ef344e2afa85d7c23fbc88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d405e1a17ad04c37ae9d488d41688639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = 'dbfs:/databricks/mlflow-tracking/3554617355181843/b66caf8248ba432aaa9accaa6df392fe/artifacts/best_model'\n",
    "loaded_model = mlflow.spark.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0006933-adb0-4336-86f0-a9f7180c35ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tipoIndex',\n",
       " 'dia_semana',\n",
       " 'mes',\n",
       " 'ano',\n",
       " 'tavg',\n",
       " 'tmin',\n",
       " 'tmax',\n",
       " 'prcp',\n",
       " 'snow',\n",
       " 'wspd',\n",
       " 'wpgt',\n",
       " 'pres',\n",
       " 'tsun']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f9a5ab-9144-47e3-8ffa-fc23e900aaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df= spark.createDataFrame([{'tipo':'laborable', 'dia_semana': 4, 'mes': 9, 'ano': 2025, 'tavg': 19.2, 'tmin': 16.2, 'tmax': 24, 'prcp':0, 'snow':0, 'wspd': 10.8, 'wpgt': 25.9, 'pres': 1015, 'tsun': 450}]) # mira, le paso un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64b626f-fa3a-4f0f-ac5b-fa85ce80a6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred = loaded_model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668cd37e-5396-4e40-aa6a-f5761d421edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ano</th><th>dia_semana</th><th>mes</th><th>prcp</th><th>pres</th><th>snow</th><th>tavg</th><th>tipo</th><th>tmax</th><th>tmin</th><th>tsun</th><th>wpgt</th><th>wspd</th><th>tipoIndex</th><th>features</th><th>prediction</th></tr></thead><tbody><tr><td>2025</td><td>4</td><td>9</td><td>0</td><td>1015</td><td>0</td><td>19.2</td><td>laborable</td><td>24</td><td>16.2</td><td>450</td><td>25.9</td><td>10.8</td><td>0.0</td><td>Map(vectorType -> dense, length -> 13, values -> List(0.0, 4.0, 9.0, 2025.0, 19.2, 16.2, 24.0, 0.0, 0.0, 10.8, 25.9, 1015.0, 450.0))</td><td>2006.2427684473732</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2025,
         4,
         9,
         0,
         1015,
         0,
         19.2,
         "laborable",
         24,
         16.2,
         450,
         25.9,
         10.8,
         0.0,
         {
          "length": 13,
          "values": [
           0.0,
           4.0,
           9.0,
           2025.0,
           19.2,
           16.2,
           24.0,
           0.0,
           0.0,
           10.8,
           25.9,
           1015.0,
           450.0
          ],
          "vectorType": "dense"
         },
         2006.2427684473732
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ano",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dia_semana",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "mes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "prcp",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pres",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "snow",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tavg",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tipo",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tmax",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tmin",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tsun",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "wpgt",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "wspd",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"laborable\",\"vispera\",\"festivo\"],\"type\":\"nominal\",\"name\":\"tipoIndex\"}}",
         "name": "tipoIndex",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":1,\"name\":\"dia_semana\"},{\"idx\":2,\"name\":\"mes\"},{\"idx\":3,\"name\":\"ano\"},{\"idx\":4,\"name\":\"tavg\"},{\"idx\":5,\"name\":\"tmin\"},{\"idx\":6,\"name\":\"tmax\"},{\"idx\":7,\"name\":\"prcp\"},{\"idx\":8,\"name\":\"snow\"},{\"idx\":9,\"name\":\"wspd\"},{\"idx\":10,\"name\":\"wpgt\"},{\"idx\":11,\"name\":\"pres\"},{\"idx\":12,\"name\":\"tsun\"}],\"nominal\":[{\"vals\":[\"laborable\",\"vispera\",\"festivo\"],\"idx\":0,\"name\":\"tipoIndex\"}]},\"num_attrs\":13}}",
         "name": "features",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":1,\"name\":\"dia_semana\"},{\"idx\":2,\"name\":\"mes\"},{\"idx\":3,\"name\":\"ano\"},{\"idx\":4,\"name\":\"tavg\"},{\"idx\":5,\"name\":\"tmin\"},{\"idx\":6,\"name\":\"tmax\"},{\"idx\":7,\"name\":\"prcp\"},{\"idx\":8,\"name\":\"snow\"},{\"idx\":9,\"name\":\"wspd\"},{\"idx\":10,\"name\":\"wpgt\"},{\"idx\":11,\"name\":\"pres\"},{\"idx\":12,\"name\":\"tsun\"}],\"nominal\":[{\"vals\":[\"laborable\",\"vispera\",\"festivo\"],\"idx\":0,\"name\":\"tipoIndex\"}]},\"num_attrs\":13}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813ac7ae-e8c2-4f2a-b367-19c94913f080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>prediction</th></tr></thead><tbody><tr><td>2006.2427684473732</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2006.2427684473732
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":1,\"name\":\"dia_semana\"},{\"idx\":2,\"name\":\"mes\"},{\"idx\":3,\"name\":\"ano\"},{\"idx\":4,\"name\":\"tavg\"},{\"idx\":5,\"name\":\"tmin\"},{\"idx\":6,\"name\":\"tmax\"},{\"idx\":7,\"name\":\"prcp\"},{\"idx\":8,\"name\":\"snow\"},{\"idx\":9,\"name\":\"wspd\"},{\"idx\":10,\"name\":\"wpgt\"},{\"idx\":11,\"name\":\"pres\"},{\"idx\":12,\"name\":\"tsun\"}],\"nominal\":[{\"vals\":[\"laborable\",\"vispera\",\"festivo\"],\"idx\":0,\"name\":\"tipoIndex\"}]},\"num_attrs\":13}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred.select('prediction').display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "668cd37e-5396-4e40-aa6a-f5761d421edb",
       "elementType": "command",
       "guid": "ff7431c1-5265-4cad-bae7-e34d2c66b643",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "d6a71d06-9ebd-44cc-a6f8-373683e40d70",
     "origId": 6973970679639030,
     "title": "Predicciones",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}